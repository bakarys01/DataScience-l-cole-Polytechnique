{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4R1Zvv2OrZWB"
   },
   "source": [
    "# TP Coding Convolutional Neural Networks in tensorflow and keras - part 2\n",
    "\n",
    "Author : Alasdair Newson\n",
    "\n",
    "alasdair.newson@telecom-paristech.fr\n",
    "\n",
    "In this session, we shall be looking at two subjects :\n",
    "\n",
    "- A way to visualise what networks are learning : the Deep Dream algorithm\n",
    "- Adversarial examples\n",
    "\n",
    "For this, we shall use a famous pretrained network : VGG16. At the heart of these applications is the calculation of the gradient of a loss function with respect to the image itself (instead of respect to the weights). The loss function will be defined depending on the application at hand.\n",
    "\n",
    "We can easily access certain well-known networks with the Keras programming framework. There are useful predefined function which allow us to load the weights, view the architecture etc. of the networks. We will specify these functions as necessary through the lab work. Unfortunately, the documentation for these functions is not very plentiful, but if you want to look at exactly what they do, you can look at the source code for help :\n",
    "\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
    "\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py\n",
    "\n",
    "However, we will indicate how these functions work as the need arises.\n",
    "\n",
    "\n",
    "First, let's load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "j33QSzfUrZWC",
    "outputId": "95757658-cf4c-4b6b-9e69-3a92e252231b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from keras.applications import vgg16\n",
    "#from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rz65_JJorZWH"
   },
   "source": [
    "You'll notice that we have loaded the 'backend'. This allows us to access certain functions of the underlying backend layer without knowing the framework-specific syntax : Keras deals with everything. As we shall see, this will be useful later on.\n",
    "\n",
    "Now, let's load one of the most famous networks, VGG16, and view it's architecture with the summary() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "VhK52zq_rZWI",
    "outputId": "efcbb78d-435d-4f89-ce11-9661a1620b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 3s 0us/step\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This removes all operations linked to training the model\n",
    "K.set_learning_phase(0)\n",
    "#load model. include_top=False means that we do not load the last fully connected layer(s) necessary for classification\n",
    "model=vgg16.VGG16(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM100CHKrZWM"
   },
   "source": [
    "The 'summary' function gives a summary of the architecture so that we can reference its different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "pPafWdysrZWN",
    "outputId": "3dbd3c8b-b145-492e-c145-5db974e5bff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QT7jSAJrZWS"
   },
   "source": [
    "You can extract one of the layers of the network with the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ooLYxVgMrZWU",
    "outputId": "574a05a7-84c6-4d91-b590-ca12eeb3772a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(512)])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put the layers in an easy-to-reference dictionary\n",
    "layer_dict = dict([ (layer.name,layer) for layer in model.layers])\n",
    "#get the layer corresponding to the 5th block and second convolution\n",
    "layer_out = layer_dict['block5_conv2'].output\n",
    "layer_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwoNPWCArZWY"
   },
   "source": [
    "You can also extract the weights of the convolutional filters in VGG16. You can do this with the following function :\n",
    "\n",
    "- weights = layer.get_weights()\n",
    "\n",
    "where 'layer' iterates throughout the 'model.layers' list. Note that this function returns the following :\n",
    "- [weights, biases] if the layer is convolutional. weights has size [m,n,filter_depth,n_filters]\n",
    "- an empty list otherwise\n",
    "\n",
    "Write a function which retrieves the weights of the network (you will have to test if the weights variable is empty at each iteration).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ip4JxQejrZWa"
   },
   "outputs": [],
   "source": [
    "def retrieve_weights(model):\n",
    "    weight_list = []\n",
    "    # BEGIN CODE HERE\n",
    "    for layer in model.layers:\n",
    "\n",
    "      # Check for convolutional layer\n",
    "      if 'conv' not in layer.name:\n",
    "        continue\n",
    "      # Get filter weights\n",
    "      filters, biases = layer.get_weights()\n",
    "\n",
    "      # checking if the weights variable is empty\n",
    "      if filters != []:\n",
    "        weight_list.append(filters)\n",
    "    # END CODE HERE\n",
    "    return weight_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hnOSDFbrZWe"
   },
   "source": [
    "Now, write a code that visualises a single channel of a filter of your choice. Reminder, to view a grey-level image, you can use :\n",
    "\n",
    "- plt.imshow(img,cmap=\"gray\")\n",
    "- plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "dGiMgD-0rZWf",
    "outputId": "275fcc7d-6546-4c2f-88af-d121f823c9b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fafe02da7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4SGpieEhqYnhIamJ4SGpyf8BHRoBeky5F1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_list = retrieve_weights(model)\n",
    "curr_weight = weight_list[2]\n",
    "curr_weight.shape\n",
    "plt.imshow(curr_weight[:,:,0,1],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ffkJHrs6rZWm",
    "outputId": "d6339b87-5c02-4033-b693-09d8dec0ebb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03781697,  0.03759558,  0.01217471],\n",
       "       [-0.01912256, -0.03680309, -0.02436409],\n",
       "       [-0.01577048, -0.00621745,  0.00700018]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_weight[:,:,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31Ai1BSgrZWq"
   },
   "source": [
    "As you can probably see, this visualisation is not of much use : we cannot really tell what is going on in the network. For this, let's turn to another approach : Deep Dream !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Gj7X9KLrZWs"
   },
   "source": [
    "## 2. Deep Dream\n",
    "\n",
    "We now proceed to carry out the Deep Dream algorithm. The idea of the Deep Dream algorithm is to find an image which maximises the response of a network at a certain layer : $\\textbf{this should help us understand what the network is learning}$. This can be done with an iterative algorithm, by simply carrying out gradient $\\textbf{ascension}$. We start with an input image and iteratively add the gradient of the average response of the features which interest us. A pseudo-code for this would be :\n",
    "\n",
    "- img = img_in\n",
    "- for i=1:n_iters\n",
    "    - img = img + grad_step $\\nabla_{img} \\mathcal{L}$,\n",
    "    \n",
    "where $\\mathcal{L}$ is the average response which interest us (you need to define this).\n",
    "\n",
    "Let's first define a function to preprocess the image. This is needed to put the image in the correct format for the VGG16 network. We also create a function to invert this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye1xSF8-rZWt"
   },
   "outputs": [],
   "source": [
    "def format_image(img_file=None):\n",
    "    \"\"\"\n",
    "    This function reads and formats an image so that it can be fed to the VGG16 network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_file : image file name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img_out_vgg : the correctly formatted image for VGG16\n",
    "    img : the image as read by the load_img function of keras.preprocessing.image\n",
    "    \"\"\"\n",
    "    # read image\n",
    "    img = load_img(img_file)\n",
    "    # convert image to an array\n",
    "    img_out = img_to_array(img)\n",
    "    # preprocess the image to put in the correct format for use with the VGG16 network trained on imagenet\n",
    "    img_out_vgg = vgg16.preprocess_input(img_out)\n",
    "    # add a dimension at the beginning, coresponding to the batch dimension\n",
    "    img_out_vgg = np.expand_dims(img_out_vgg, axis=0)\n",
    "    return img_out_vgg, img\n",
    "\n",
    "def unformat_image(img_in):\n",
    "    \"\"\"\n",
    "    This function inverts the preprocessing applied to images for use in the VGG16 network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_file : formatted image of shape (batch_size,m,n,3)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img_out : a m-by-n-by-3 array, representing an image that can be written to an image file\n",
    "    \"\"\"\n",
    "    #get rid of batch dimension\n",
    "    img_out=np.squeeze(img_in)\n",
    "    #remove offsets added by the VGG16 preprocessing\n",
    "    img_out[:, :, 0] += 103.939\n",
    "    img_out[:, :, 1] += 116.779\n",
    "    img_out[:, :, 2] += 123.68\n",
    "    # invert the order of the colours : BGR -> RGB\n",
    "    img_out = img_out[:, :, ::-1]\n",
    "    #clamp image to the range [0,255] and cast to uint8\n",
    "    img_out = np.clip(img_out, 0, 255).astype('uint8')\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dm9uklN-rZWw"
   },
   "source": [
    "Now, we load the image. At the same time, we create a backend Tensor which has the correct format for the network.\n",
    "\n",
    "**NOTE** : if you are using colab, then you might not be able to easily upload the images of the lab work. In this case you can use download the image directly from the url given (this is done for you). To modify the behaviour, change the ```using_colab``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "lMHAlLiNrZWx",
    "outputId": "3921764f-e0a2-4a2c-c46e-017ae87b49b7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a0a990fac33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimg_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#show the input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_visu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_visu' is not defined"
     ]
    }
   ],
   "source": [
    "using_colab = False\n",
    "\n",
    "#load image\n",
    "if (using_colab == True):\n",
    "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg\"\n",
    "    img_in,img_visu = format_image('got.jpg')\n",
    "else:\n",
    "    img_in,img_visu = format_image('images/got.jpg')\n",
    "#create a Tensor in the correct format for the network using the backend framework (Keras in this case)\n",
    "img_backend = model.input\n",
    "#show the input image\n",
    "plt.imshow(img_visu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSwJ-iy-rZW1"
   },
   "source": [
    "Now, we define the loss that we wish to maximise. This can be anything you wish, but the a common loss is simply the average response of a certain channel of a certain layer. Since these responses are all positive, due to the non-linearities used, we can safely take the average as a loss function to maximise.\n",
    "\n",
    "Define the loss as the average response of the 15th channel of the 5th layer, second convolution. Then define the gradients of loss with respect to the image.\n",
    "\n",
    "Note that all these operations should be done using the backend functionalities, since we are working on symbolic Tensors which have not yet been given numeric data. For example, use the following functions :\n",
    "- K.mean()\n",
    "- K.gradients(loss, image_variable)[0]  #we take the [0]th element here because the gradients function necessarily returns a list, even when the length of this list is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXwMIXkZrZW2"
   },
   "outputs": [],
   "source": [
    "#BEGIN STUDENT CODE\n",
    "layer_out = layer_dict['block5_conv2'].output # filter of the 5th layer: 2nd convolutional layer\n",
    "#define loss\n",
    "loss = K.mean(layer_out[:,:,:,14])\n",
    "grads = K.gradients(loss, img_backend)[0]\n",
    "#END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_w6MCQc5rZW7"
   },
   "source": [
    "Here, we use a little trick. Indeed, it may be the case that the gradients are far too small or far too large for updating, meaning that the updates do nothing or destroy the image. To avoid this, we normalise the gradients. Normalise the gradient in the following manner :\n",
    "\n",
    "- grad = grad/(max(mean(abs(grad)))\n",
    "\n",
    "Again, you must use the backend functions to manipulate the gradient (a Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ooRe3petrZW8"
   },
   "outputs": [],
   "source": [
    "# normalize the gradient\n",
    "grads_normalised = grads/ (K.maximum(K.mean(K.abs(grads)), K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCHwJvVXrZXA"
   },
   "source": [
    "Now, we need to define a function to retrieve the loss and gradients so as to maximise our loss function. This can done easily in Keras with the following syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oMRML65rZXB"
   },
   "outputs": [],
   "source": [
    "get_loss_and_grads = K.function([img_backend],[loss,grads_normalised])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxyGIDuKrZXE"
   },
   "source": [
    "We are now ready to carry out the Deep Dream algorithm using gradient ascent, yipee ! Iterate 'n_iterations' times, each time adding an epsilon of the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "ytVh3eHnrZXF",
    "outputId": "1a42476b-df23-477a-b999-bc88dd2d8367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-17 13:12:57--  https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg\n",
      "Resolving perso.telecom-paristech.fr (perso.telecom-paristech.fr)... 137.194.2.165, 2001:660:330f:2::a5\n",
      "Connecting to perso.telecom-paristech.fr (perso.telecom-paristech.fr)|137.194.2.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 502 Bad Gateway\n",
      "2019-11-17 13:13:00 ERROR 502: Bad Gateway.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-613dd5785b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musing_colab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget \"https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'got.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/got.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-459a65217b53>\u001b[0m in \u001b[0;36mformat_image\u001b[0;34m(img_file)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# read image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# convert image to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'got.jpg'"
     ]
    }
   ],
   "source": [
    "# first, reload image to make sure that we are not starting from a previous initialisation\n",
    "if (using_colab == True):\n",
    "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg\"\n",
    "    img_in,_ = format_image('got.jpg')\n",
    "else:\n",
    "    img_in,_ = format_image('images/got.jpg')\n",
    "    \n",
    "step = 0.1 # Gradient ascent step size\n",
    "n_iterations = 100  # Number of gradient ascent steps\n",
    "for ii in range(0,n_iterations):\n",
    "    loss_value,grads_value = get_loss_and_grads([img_in])\n",
    "    img_in = img_in + grads_value * step\n",
    "    if (ii%5==0):\n",
    "        print(\".\", end='')\n",
    "        img_out = unformat_image(np.copy(img_in))\n",
    "        plt.imshow(img_out)\n",
    "        plt.imsave('img_out_'+str(ii).zfill(3)+'.png',img_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxC9i9OGrZXJ"
   },
   "source": [
    "You can try different convolutional layers and see what the results are !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BANlnYGOrZXK"
   },
   "source": [
    "## 3. Adversarial examples\n",
    "\n",
    "In this part of the lab work, we will explore the interesting case of adversarial examples. Adversarial examples are images which have been perturbed in a manner which makes the network misclassify the image.\n",
    "\n",
    "There are many ways to do this, however we can use a similar approach to the one used above, that is to say, we will use a gradient maximisation approach. This consists in iteratively adding the gradient of the loss with respect to the image, to the current image, in order to get a misclassified image.\n",
    "\n",
    "For this application, we need access to the last, classification, layer of the VGG16 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "XR8h8nDOrZXL",
    "outputId": "c345c0e1-9afa-424d-ebf1-4bec40479dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 17s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model, including last (classification layer)\n",
    "model=vgg16.VGG16(weights='imagenet',include_top=True)\n",
    "#create layer dictionary\n",
    "layer_dict = dict([ (layer.name,layer) for layer in model.layers])\n",
    "#create backend Tensor\n",
    "img_backend = model.input\n",
    "# display architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BQmBqTNrZXP"
   },
   "source": [
    "We are going to take an image of a cat and try to misclassify it. First, read and format the image.\n",
    "\n",
    "NOTE !!!\n",
    "\n",
    "In the previous code, we were not interested in the fully connected layers, so the input image could be of any size (there were only convolutional and maxpool layers). Now, since we are using the fully connected layers, we need to make sure the input is of the correct size for the images in imagenet : 224x224. For this, we redefine the format_image function to force the image to a certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DVoIiIYrZXQ"
   },
   "outputs": [],
   "source": [
    "def format_image_classif(img_file,img_width=224,img_height=224):\n",
    "    \"\"\"\n",
    "    This function reads and formats an image so that it can be fed to the VGG16 network.\n",
    "    In this case, we wish to force the image size to a certain shape, since we want to use the image for\n",
    "    classification\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_file : image file name\n",
    "    img_width : the target image width\n",
    "    img_height : he target image height\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img_out_vgg : the correctly formatted image for VGG16\n",
    "    img : the image as read by the load_img function of keras.preprocessing.image\n",
    "    \"\"\"\n",
    "    # read image. Force the image size to a certain shape (uses a resize of the pillow package)\n",
    "    img = load_img(img_file,target_size=(img_height,img_width))\n",
    "    # convert image to an array\n",
    "    img_out = img_to_array(img)\n",
    "    # preprocess the image to put in the correct format for use with the VGG16 network trained on imagenet\n",
    "    img_out_vgg = vgg16.preprocess_input(img_out)\n",
    "    # add a dimension at the beginning, coresponding to the batch dimension\n",
    "    img_out_vgg = np.expand_dims(img_out_vgg, axis=0)\n",
    "    return img_out_vgg, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "ZGEO6TLnrZXU",
    "outputId": "80f521d8-7598-4040-fa19-b4934158347a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-17 13:13:41--  https://perso.telecom-paristech.fr/anewson/doc/images/cat_small.png\n",
      "Resolving perso.telecom-paristech.fr (perso.telecom-paristech.fr)... 137.194.2.165, 2001:660:330f:2::a5\n",
      "Connecting to perso.telecom-paristech.fr (perso.telecom-paristech.fr)|137.194.2.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 502 Bad Gateway\n",
      "2019-11-17 13:13:44 ERROR 502: Bad Gateway.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5efa244d4a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musing_colab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget \"https://perso.telecom-paristech.fr/anewson/doc/images/cat_small.png\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_visu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_image_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat_small.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_visu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_image_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/cat_small.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c77ea61da612>\u001b[0m in \u001b[0;36mformat_image_classif\u001b[0;34m(img_file, img_width, img_height)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# read image. Force the image size to a certain shape (uses a resize of the pillow package)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# convert image to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mimg_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cat_small.png'"
     ]
    }
   ],
   "source": [
    "if (using_colab == True):\n",
    "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/cat_small.png\"\n",
    "    img_in,img_visu = format_image_classif('cat_small.png')\n",
    "else:\n",
    "    img_in,img_visu = format_image_classif('images/cat_small.png')\n",
    "\n",
    "plt.imshow(img_visu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fzdE2pqrZXd"
   },
   "outputs": [],
   "source": [
    "img_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtTNTT2erZXn"
   },
   "source": [
    "Now, we need to retrieve the last (prediction) layer of the VGG network. Do this via the variable layer_dict created above, and calling the correct layer (similarly to when we called the 'block5_conv2' layer above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "da0lVXm5rZXp"
   },
   "outputs": [],
   "source": [
    "#BEGIN STUDENT CODE\n",
    "#get last layer (prediction layer)\n",
    "last_layer = layer_dict['predictions'].output\n",
    "#create a function to get last layer of the VGG16 network (using the backend function)\n",
    "get_prediction_layer = ...\n",
    "#END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtxEYq3NrZXs"
   },
   "source": [
    "We are going to try to force the image to recognise a 'reflex_camera'. This is number 759 of the imagenet classes. You can use any one you like in fact (apart from ones linked to cats, obviously). To see the list of classes go to :\n",
    "\n",
    "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "\n",
    "Now, find the initial class of the cat image. Also display the top 5 classification results. For this, you can use the following functions :\n",
    "\n",
    "- model.predict(img) : classification prediction of the img variable\n",
    "- vgg16.decode_predictions(y, top=5)[0] : converts the numerical probabilities in the y variable to human readable classes. Here,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsNQhul7rZXt"
   },
   "outputs": [],
   "source": [
    "target_class = 759\n",
    "# BEGIN STUDENT CODE\n",
    "# carry out the network predictions on the example image\n",
    "y_predicted = model.predict(img_to_array)\n",
    "#define the true class as the initial most likely class\n",
    "true_class = ...\n",
    "# print the top 5 predicted classes, with the prediction probability\n",
    "y_predicted_decoded = ...\n",
    "print(y_predicted_decoded)\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rh-Qr5f1rZXy"
   },
   "source": [
    "Now, redefine the loss to be the 'target_class' element of the last layer of the network. This is the element we shall try to maximise, in order to fool the network. Redefine also the (normalised) gradient as above, and create a function to extract the loss and the gradients, given an input image as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRwghCKhrZX1"
   },
   "outputs": [],
   "source": [
    "#BEGIN STUDENT CODE\n",
    "# define loss and gradients\n",
    "\n",
    "# normalize the gradient\n",
    "\n",
    "# create function to retrieve loss and gradients of loss with respect to image\n",
    "\n",
    "#END STUDENT CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Q_hOzxUrZX4"
   },
   "source": [
    "We are now ready to perturb the image such that we misclassify it. Youhoo !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fk4BM6-XrZX5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reload image, to make sure we are not starting from previous point\n",
    "img_in,_ = format_image_classif('images/cat_small.png')\n",
    "\n",
    "#parameters\n",
    "step = 0.5  # Gradient ascent step size\n",
    "n_iterations = 100  # Number of gradient ascent steps\n",
    "for ii in range(0,n_iterations):\n",
    "    loss_value,grads_value = get_loss_and_grads_class([img_in])\n",
    "    img_in = img_in + grads_value * step\n",
    "    if (ii%5==0):\n",
    "        img_show = unformat_image(np.copy(img_in))\n",
    "        plt.imsave('img_out_'+str(ii).zfill(3)+'.png',img_show)\n",
    "        #predict current model to see evolution of top classification\n",
    "        y_predicted = model.predict(img_in)\n",
    "        print(vgg16.decode_predictions(y_predicted, top=1)[0])\n",
    "print('End of optimisation')\n",
    "# finally, display misclassified image\n",
    "img_show = unformat_image(np.copy(img_in))\n",
    "plt.imshow(img_show)\n",
    "#show the final top 5 classes\n",
    "y_predicted = model.predict(img_in)\n",
    "print(vgg16.decode_predictions(y_predicted, top=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKDv7BkwrZX9"
   },
   "source": [
    "As you should probably see, the image is changed such that it is no longer is correctly classified. It should be classified as a 'reflex camera', or whatever you chose, with high probability. This is a problem, since a human is still able to see a cat ! Furthermore, the top 5 classifications have nothing to do with cats !! Even bigger problem !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsVGEO_ErZX-"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for :\n",
    "- 1) Deep dream, ```retrieve_weights``` : 2 points\n",
    "- 2) Deep dream, calculation of gradients (```loss``` and ```grads```) : 2 points\n",
    "- 3) Adversarial examples, ```last_layer``` and ```get_prediction_layer``` : 2 points\n",
    "- 4) Adversarial examples, ```y_predicted```, ```true_class``` and ```y_predicted_decoded```\n",
    "- 5) Adversarial examples, get loss, gradient, normalise gradient, create function to retrieve the loss and gradients : 2 points\n",
    "\n",
    "Each correct answer (correct formula/code and code runs) gives 2 points. Total over 10 points."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3tp_deep_learning_cnn_part_2_for_students.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPg2FAMYEpaB"
   },
   "source": [
    "# PROJECT: Course Optimization for Data Science\n",
    "## Optimization strategies for robust regression\n",
    "\n",
    "\n",
    "Author: Alexandre Gramfort\n",
    "\n",
    "If you have questions or if something is not clear in the text below please contact us\n",
    "by email.\n",
    "\n",
    "## Aim:\n",
    "\n",
    "- Derive mathematically and implement the loss and gradients of the Huber model\n",
    "- Implement your own solvers for L1 or squared L2 regularization with: (Accelerated) Proximal gradient descent, Proximal coordinate descent and L-BFGS (only for L2).\n",
    "- Implement your own scikit-learn estimator for the Huber model and test it against the Ridge and Lasso from scikit-learn on some real data.\n",
    "- You are expected to provide clear figures as one could expect from an experiment section in a research paper.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "This work must be done by pairs of students.\n",
    "Each student must send their work before the 31st of January at 23:59, using the moodle platform.\n",
    "This means that **each student in the pair sends the same file**\n",
    "\n",
    "On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called \"Project\".\n",
    "This is where you submit your jupyter notebook file.\n",
    "\n",
    "The name of the file must be constructed as in the next cell\n",
    "\n",
    "### Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "#### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Wemeg52QEpaF",
    "outputId": "67b6c9ed-154e-4cf2-f187-42a48d3d7d8b"
   },
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"sidibe\"\n",
    "ln1 = \"bakary\"\n",
    "fn2 = \"adnan\"\n",
    "ln2 = \"asadullah\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"project\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6GQm-jlEpaL"
   },
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvoeqOUZEpaM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hHVNEQUEpaQ"
   },
   "source": [
    "## Part 0: Why a robust regression model\n",
    "\n",
    "\n",
    "Let us consider the problem of regression from $n$ observations\n",
    "$x_i \\in \\mathbb{R}^{p}$,\n",
    "$1 \\leq i \\leq n$. We aim to learn a function:\n",
    "$$f: x \\in \\mathbb{R}^{p}\\mapsto y\\in\\mathbb{R}$$\n",
    "from the $n$ annotated training samples $(x_{i},y_{i})$ supposed i.i.d. from an unknown probability distribution on $\\mathbb{R}^p \\times \\mathbb{R}$. Once this function is learnt, it will be possible to use it to predict the label $y$ associated to a new sample $x$.\n",
    "\n",
    "The types of model we consider in this project are so-called *robust models* that can deal with samples corrupted by strong artifacts.\n",
    "\n",
    "Let's generate such a dataset in 1D to illustrate the problem when using the squared loss ($\\|\\cdot\\|^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "QZ3iuU-zEpaR",
    "outputId": "ed05c29a-32bc-41c8-a395-e5533bc3f4ac"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate toy data\n",
    "X, y = make_regression(n_samples=20, n_features=1, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "\n",
    "# Add an outlier\n",
    "X[0, 0] = 2.\n",
    "y[0] = 350\n",
    "\n",
    "# Fit the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Visualize the model\n",
    "x = X[:, 0]\n",
    "y_pred = reg.coef_ * x + reg.intercept_\n",
    "\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.plot(x, y_pred, 'g-', label=\"linear regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50eMX1VkEpaV"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 0:</b>\n",
    "     <ul>\n",
    "       <li>Describe the issue you observe and suggest an explanation and a possible solution.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCiFOisYEpaW"
   },
   "source": [
    "INSERT YOUR ANSWER HERE\n",
    "\n",
    "- We observe that there is an outlier which affects the prediction of the model, meaning, the model (with squared loss) prediction is dragged towards the outlier.\n",
    "- The simplest solution is to use a model with least absolute deviations, which is less sensitive to outliers than least squares estimates in Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iC3wW9oREpaX"
   },
   "source": [
    "## Part 1: Huber Loss\n",
    "\n",
    "One version of the Huber function ($H_\\epsilon : \\mathbb{R} \\rightarrow \\mathbb{R}$) reads:\n",
    "\n",
    "$$\n",
    "    H_\\epsilon (x) = \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\tx^2 & \\quad \\mathrm{ if } \\quad |x| < \\epsilon \\\\\n",
    "    2 \\epsilon |x| - \\epsilon^2 & \\quad \\mathrm{ otherwise }\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Working in a regression setting, the Huber loss between 2 targets $y$ and $y'$ reads:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(y, y') = H_\\epsilon (y - y')\n",
    "$$\n",
    "\n",
    "Here is an implemention of the Huber function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7aiu0fOEpaY"
   },
   "outputs": [],
   "source": [
    "epsilon = 1.\n",
    "\n",
    "def huber(x, epsilon=epsilon):\n",
    "    mask = np.abs(x) < epsilon\n",
    "    z = x.copy()\n",
    "    z[mask] = x[mask] ** 2\n",
    "    z[~mask] = 2 * epsilon * np.abs(x[~mask]) - epsilon ** 2\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYNujHGDEpad"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b>\n",
    "     <ul>\n",
    "       <li>Plot the Huber function vs. the squared function ($x \\rightarrow x^2$) vs. the absolute value function ($x \\rightarrow |x|$) between -3 and 3 using $\\epsilon = 1$</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "yl3oo-XaEpag",
    "outputId": "1ded0cf7-e4c3-4b23-ea3b-26f0a74e0a0a"
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "x = np.linspace(-3,3, 50)\n",
    "\n",
    "huber_x = huber(x, epsilon=1)\n",
    "x_square = x*x\n",
    "abs_x = np.abs(x)\n",
    "\n",
    "plt.plot(x, huber_x, label='huber')\n",
    "plt.plot(x, x_square, label='$x^2$')\n",
    "plt.plot(x, abs_x, label='$|x|$')\n",
    "plt.legend()\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gK_bfizFEpaj"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "       <li>Justify the convexity of the Huber function as defined above.</li>\n",
    "       <li>Justify the smoothness of the Huber function as defined above and propose a value of for the Lipschitz constant of its gradient.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o55jzsLDEpal"
   },
   "source": [
    "INSERT YOUR ANSWER HERE\n",
    "\n",
    "- Convexity of Huber loss function:\n",
    "\n",
    "We say that  a twice differentiable function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ is convex if \n",
    "\n",
    "$$\n",
    "f(\\lambda x+(1-\\lambda) y) \\leq \\lambda f(x)+(1-\\lambda) f(y), \\quad \\forall x, y \\in \\mathbb{R}^{d}, \\lambda \\in[0,1]\n",
    "$$\n",
    "or equivalently\n",
    "$$\n",
    "v^{\\top} \\nabla^{2} f(x) v \\geq 0, \\quad \\forall x, v \\in \\mathbb{R}^{d}\n",
    "$$\n",
    "\n",
    "> Proof for Huber loss\n",
    "\n",
    "$$\n",
    "    \\nabla H_\\epsilon (x) = \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\t2 x & \\quad \\mathrm{ if } \\quad |x| < \\epsilon \\\\\n",
    "    2 \\epsilon  * sign(x)  & \\quad \\mathrm{ otherwise }\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\nabla^2 H_\\epsilon (x) = 2\n",
    "$$\n",
    "\n",
    "> $ \\nabla^2 H_\\epsilon (x) \\geq 0  $, so, the Huber function is convex.\n",
    "\n",
    "\n",
    "- Smoothness of Huber function:\n",
    "\n",
    "We say that a function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ is L-smooth if \n",
    "\n",
    "$$\n",
    " \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\|\n",
    "$$\n",
    "\n",
    "Or equivalently if f is twice differentiable, then\n",
    "\n",
    "$$\n",
    "v^{\\top} \\nabla^{2} f(x) v \\leq L\\|v\\|_{2}^{2}, \\quad \\forall x, v \\in \\mathbb{R}^{d}\n",
    "$$\n",
    "> We have that $\\nabla^2 H_\\epsilon (x) = 2 \\Leftrightarrow  v^{\\top} \\nabla^2 H v \\leq 2\\|v\\|_{2}^{2}$, so the Huber function is 2-Smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhlGcF47Epam"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b>\n",
    "     <ul>\n",
    "       <li>Write a function that computes the gradient of the Huber loss.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** You will use the `scipy.optimize.check_grad` function to assess the validity of your result. You will need to test your gradient in both the linear and quadratic regions of the Huber function (not just in one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "9ooZUT9cEpao",
    "outputId": "2abc436c-663b-4f51-9e36-42b5fc6b0051"
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "from scipy.optimize import check_grad\n",
    "def grad_huber(x, epsilon=epsilon):\n",
    "    \n",
    "    mask = np.abs(x) < epsilon\n",
    "    g = x.copy()\n",
    "    g[mask] = 2 * x[mask] #if |x| < eps\n",
    "    g[~mask] = 2 * epsilon * np.sign(x[~mask]) #if |x| >= eps\n",
    "    \n",
    "    return g\n",
    "\n",
    "\n",
    "# Grad  check\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "print('Check the Huber gradient :')\n",
    "[check_grad(huber, grad_huber,x0 = np.array([value])) for value in np.linspace(-3,3, 10)]\n",
    "\n",
    "### END TODO    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijMW7EY5Epau"
   },
   "source": [
    "Let us define the cost function associated to the empirical risk with some regularization function $\\mathcal{R}$:\n",
    "\n",
    "$$\n",
    "    (\\mathcal{P}_{f,\\mathcal{R}}):\n",
    "\t\\begin{aligned}\n",
    "\t\\min_{w \\in \\mathbb{R}^p, b \\in \\mathbb{R}} \\quad \\frac{1}{n} \\sum_{i=1}^n f(y_i - x_i^\\top w - b) + \\lambda \\mathcal{R}(w) \\enspace ,\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $f$ is a scalar function defining the loss (Huber, squared, absolute etc.). The variable $b$ is the bias or intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ec-nwyHXEpax"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b>\n",
    "     <ul>\n",
    "      <li>Let us consider for $\\mathcal{R}$ either the $\\ell_1$ norm ($\\mathcal{R}_1(w) = \\|w\\|_1 = \\sum_{j=1}^p |w_j|$) or the squared $\\ell_2$ norm ($\\mathcal{R}_2(w) = \\|w\\|_2^2 = \\sum_{j=1}^p w_j^2)$. Justify what optimization strategy among L-BFGS, (proximal-)gradient descent, (proximal-)coordinate descent is readily applicable, depending on the choice of $\\mathcal{R}$ when $f$ is the Huber function as defined above.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBZm1rPVEpaz"
   },
   "source": [
    "INSERT YOUR ANSWER HERE\n",
    "\n",
    "- For proximal-gradient descent to converge, we need a convex regularization function $\\mathcal{R}$ and a differentiable, smooth and convex loss function (Beck Teboulle 2009).\n",
    "The Huber function is differentiable, smooth and convex.\n",
    "Every norm is convex.\n",
    "Therefore, proximal-gradient descent is readily applicable and would converge whatever the norm used.\n",
    "\n",
    "- For the convergence of gradient descent, we also require the regularizor to be smooth. While the $\\ell_2$ norm is smooth, the $\\ell_1$ isn't.\n",
    "\n",
    "- For (proximal-)coordinate descent, we need $f$ to be convex and differentiable, $\\nabla f$ to be Lipschitz continuous it is the case for the Huber function (because $f$ has values in $\\mathbb{R}$ and is smooth for the third point) .\n",
    "\n",
    "- For L-BFGS algorithm, the $\\ell_2$ norm is better suited than the $\\ell_1$ norm for the quadratic approximation and it is twice differentiable as the Huber function. The Hessian would also be positive definite and therefore the convergence would be ensured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYWj96flEpa0"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 5:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Taking as $f$ the Huber function and the $\\mathcal{R}_2$ regularization function, solve the optimization prolem $(\\mathcal{P}_{H_\\epsilon,\\mathcal{R}_2})$ using the `fmin_l_bfgs_b` function from `scipy.optimize`. You are expected to provide the explicit gradient (fprime parameter) to `fmin_l_bfgs_b`.\n",
    "    </li>\n",
    "    <li>Using the simulated dataset from above, you will check that your solver fixes the problem of the outlier provided that $\\lambda$ is small enough (eg. $\\lambda = 0.01$). Your are expected to make a plot of the regression fit.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "The estimate of $w$ and $b$ should be called `w_hat` and `b_hat`. You will call the regularization parameter $\\lambda$ as `lbda` in the code.\n",
    "\n",
    "To help you we provide you with the function `pobj_l2` that computes the primal objective to minimize. Note that the parameters `w` and `b` are combined in a single array `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cu7OKCZKEpa1"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "lbda = 0.01\n",
    "\n",
    "def pobj_l2(params, X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    return np.mean(huber(y - np.dot(X, w) - b, epsilon=epsilon)) + lbda * np.sum(w ** 2)\n",
    "  \n",
    "\n",
    "def grad_huber_loss(params, X=X, y=y, epsilon=epsilon): #explicit gradient (fprime parameter) without regularizor\n",
    "  \n",
    "  n = X.shape[0]\n",
    "  w = params[1:]\n",
    "  b = params[0]\n",
    "  \n",
    "  grad = params.copy()\n",
    "  \n",
    "  grad[1:] = - X.T.dot(grad_huber(y - X.dot(w) - b, epsilon))/n #grad for w\n",
    "  grad[0] = - np.sum(grad_huber(y - X.dot(w) - b, epsilon))/n #grad for b (intercept)\n",
    "  \n",
    "  return grad\n",
    "  \n",
    "def grad_pobj_12(params, X=X, y=y, lbda=lbda, epsilon=epsilon): #explicit gradient (fprime parameter) with regularizor\n",
    "\n",
    "  w =  params[1:]\n",
    "  \n",
    "  grad = params.copy()\n",
    "  grad[1:] = grad_huber_loss(params, X=X, y=y, epsilon=epsilon)[1:] + 2 * lbda * w #grad for w\n",
    "  grad[0] = grad_huber_loss(params, X=X, y=y, epsilon=epsilon)[0] #grad for b\n",
    " \n",
    "  return grad\n",
    "                 \n",
    "                 \n",
    "def huber_lbfgs_l2(X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    # TODO\n",
    "    n,p = np.shape(X)\n",
    "    init_params = np.ones(p+1)\n",
    "    params, f, d = fmin_l_bfgs_b(func=pobj_l2, x0=init_params, fprime=grad_pobj_12, args=(X,y,lbda, epsilon))\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "mwzTmKirMKiF",
    "outputId": "59e96870-be79-4848-a73b-afeb0799ea4d"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "params = huber_lbfgs_l2(X=X, y=y, lbda=lbda, epsilon=epsilon)\n",
    "\n",
    "w_hat = params[1:]\n",
    "b_hat = params[0]\n",
    "x = X[:, 0]\n",
    "y_pred = w_hat * x + b_hat\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, 'r')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression with Huber loss')\n",
    "plt.show()\n",
    "\n",
    "# END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNFkuPayyjtq"
   },
   "source": [
    "We can see that the linear regression is not affected any more by the outlier with the new solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WaQhadADEpa5"
   },
   "source": [
    "## Part 2: Huber Loss with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3K0em2f5Epa6"
   },
   "source": [
    "In this section we are interested in the $\\ell_1$ regularized model.\n",
    "To help you we give you the code of the objective function to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHlLPr1KEpa7"
   },
   "outputs": [],
   "source": [
    "def pobj_l1(params, X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    return np.mean(huber(y - np.dot(X, w) - b, epsilon=epsilon)) + lbda * np.sum(np.abs(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEqiiW0jEpbB"
   },
   "source": [
    "Now that we have the cost function, you are going to implement solvers based on:\n",
    "\n",
    "- Proximal Gradient Descent (PGD aka ISTA)\n",
    "- Accelerated Proximal Gradient Descent (APGD aka FISTA)\n",
    "- Proximal Coordinate Descent (PCD)\n",
    "\n",
    "Before this we are going to define the `monitor` class previously used in the second lab as well as plotting functions useful to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq7WMr-HEpbD"
   },
   "outputs": [],
   "source": [
    "class monitor(object):\n",
    "    def __init__(self, algo, obj, x_min, args=()):\n",
    "        self.x_min = x_min\n",
    "        self.algo = algo\n",
    "        self.obj = obj\n",
    "        self.args = args\n",
    "        if self.x_min is not None:\n",
    "            self.f_min = obj(x_min, *args)\n",
    "\n",
    "    def run(self, *algo_args, **algo_kwargs):\n",
    "        t0 = time.time()\n",
    "        _, x_list = self.algo(*algo_args, **algo_kwargs)\n",
    "        self.total_time = time.time() - t0\n",
    "        self.x_list = x_list\n",
    "        if self.x_min is not None:\n",
    "            self.err = [linalg.norm(x - self.x_min) for x in x_list]\n",
    "            self.obj = [self.obj(x, *self.args) - self.f_min for x in x_list]\n",
    "        else:\n",
    "            self.obj = [self.obj(x, *self.args) for x in x_list]\n",
    "\n",
    "\n",
    "def plot_epochs(monitors, solvers):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    for monit in monitors:\n",
    "        ax1.semilogy(monit.obj, lw=2)\n",
    "        ax1.set_title(\"Objective\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        if monit.x_min is None:\n",
    "            ax1.set_ylabel(\"$f(x_k)$\")\n",
    "        else:\n",
    "            ax1.set_ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    ax1.legend(solvers)\n",
    "\n",
    "    for monit in monitors:\n",
    "        if monit.x_min is not None:\n",
    "            ax2.semilogy(monit.err, lw=2)\n",
    "            ax2.set_title(\"Distance to optimum\")\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.set_ylabel(\"$\\|x_k - x^*\\|_2$\")\n",
    "\n",
    "    ax2.legend(solvers)\n",
    "\n",
    "\n",
    "def plot_time(monitors, solvers):\n",
    "    for monit in monitors:\n",
    "        objs = monit.obj\n",
    "        plt.semilogy(np.linspace(0, monit.total_time, len(objs)), objs, lw=2)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Timing\")\n",
    "        plt.ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    plt.legend(solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bAj6wQ5EpbK"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6a:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the proximal gradient descent (PGD) method.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Note:**  The parameter `step` is the size of the gradient step that you will need to propose by computing the Lipschitz constant of the data fitting term (Huber term without regularization term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDJ57xbLEpbL"
   },
   "outputs": [],
   "source": [
    "def pgd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Proximal gradient descent algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_init : array, shape (n_parameters,)\n",
    "        Parameters of the optimization problem.\n",
    "    grad : callable\n",
    "        The gradient of the smooth data fitting term.\n",
    "    prox : callable\n",
    "        The proximal operator of the regularization term.\n",
    "    step : float\n",
    "        The size of the gradient step done on the smooth term.\n",
    "    n_iter : int\n",
    "        The number of iterations.\n",
    "    store_every : int\n",
    "        At which frequency should the current iterated be remembered.\n",
    "    grad_args : tuple\n",
    "        Parameters to pass to grad.\n",
    "    prox_args : tuple\n",
    "        Parameters to pass to prox.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : array, shape (n_parameters,)\n",
    "        The estimated parameters.\n",
    "    x_list : list\n",
    "        The list if x values along the iterations.\n",
    "    \"\"\"\n",
    "    x = x_init.copy()\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "        \n",
    "#         x = prox(x - step * grad(x, grad_args[0], grad_args[1], grad_args[2]), step, *prox_args)\n",
    "        x = prox(x - step * grad(x, *grad_args),step, *prox_args) #We apply Prox-GD step\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvCDHwIGEpbS"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6b:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the L1 and L2 proximal operators. You will pay attention to the intercept.\n",
    "    </li>\n",
    "    <li>\n",
    "        Using the monitor class and the plot_epochs function, display the convergence.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "In order to get a good value of `x_min` you will let your PGD solver run for 10000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tfxx6dK5EpbU"
   },
   "source": [
    "First you will need to implement the proximal operator functions for $\\ell_1$ and $\\ell_2$ regularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrIeYAuaTuCz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydEo009gEpbV"
   },
   "outputs": [],
   "source": [
    "def prox_R2(params, reg=1.):\n",
    "    # TODO\n",
    "    \n",
    "    params[1:] = params[1:]/(1+2*reg)\n",
    "    \n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_R1(params, reg=1.):\n",
    "    # TODO\n",
    "    params[1:]= np.sign(params[1:]) * np.maximum(np.abs(params[1:])- reg,0)\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_l2(params, step, lbda):\n",
    "    return prox_R2(params, reg=step * lbda)\n",
    "\n",
    "\n",
    "def prox_l1(params, step, lbda):\n",
    "    return prox_R1(params, reg=step * lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0y0ilwAcc_T9"
   },
   "outputs": [],
   "source": [
    "def smoothness_const_huber(A): #Lipschitz constant of the data fitting term (Huber term without regularization term) => step = 1/L\n",
    "  n = A.shape[0]\n",
    "  l = np.linalg.norm(A, ord=2)**2\n",
    "  \n",
    "  return 2*l/n #We have shown that the Huber function is 2-smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3_gBeOcEpbZ"
   },
   "outputs": [],
   "source": [
    "# Generate bigger data\n",
    "X, y = make_regression(n_samples=500, n_features=100, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "# Set initial values of parameters to optimize\n",
    "x_init = np.zeros(n_features + 1)\n",
    "x_init[0] = np.mean(y)\n",
    "n_iter = 2000\n",
    "lbda = 1.\n",
    "epsilon = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "mzYX8gWP7QN4",
    "outputId": "ea522aa0-bcfd-456e-bac1-37149b6fff25"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "step = 1/smoothness_const_huber(X)\n",
    "\n",
    "x_min_l2, x_list_l2 = pgd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "x_min_l1, x_list_l1 = pgd(x_init, grad_huber_loss, prox_l1, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "\n",
    "# END TODO\n",
    "\n",
    "# Run PGD\n",
    "monitor_pgd_l2 = monitor(pgd, pobj_l2, x_min_l2, args=(X, y, lbda, epsilon))\n",
    "monitor_pgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "monitors = [monitor_pgd_l2]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtR8BeC2Epbc"
   },
   "source": [
    "Now for the $\\ell_1$ regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "QZRU4SWnEpbd",
    "outputId": "500e130e-bd46-4b50-ac85-c8b5858211a7"
   },
   "outputs": [],
   "source": [
    "# Run PGD for L1\n",
    "monitor_pgd_l1 = monitor(pgd, pobj_l1, x_min=x_min_l1, args=(X, y, lbda, epsilon))\n",
    "monitor_pgd_l1.run(x_init, grad_huber_loss, prox_l1, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "monitors = [monitor_pgd_l1]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeQu7AILEpbi"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 7:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the accelerated proximal gradient descent (APGD) and add this solver to the monitoring plots.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8_O8HcxEpbk"
   },
   "outputs": [],
   "source": [
    "def apgd(x_init, grad, prox,step=1., n_iter=100,  store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Accelerated proximal gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    z = x_init.copy()\n",
    "    t = 1.\n",
    "    t_new = t\n",
    "    \n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        x_new = prox(z - step * grad(z,*grad_args),step, *prox_args) #params = (x), step = step, prox_args = (lbda,)\n",
    "        t_new = (1 + np.sqrt(1 + 4 * (t**2)))/2\n",
    "        z = x_new + (t-1)/(t_new) * (x_new - x)\n",
    "        \n",
    "        x = x_new.copy()\n",
    "        t = t_new\n",
    "    \n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "EKzSTzOCEpbq",
    "outputId": "b747dda8-c6d3-426e-e41d-e388d207c3fa"
   },
   "outputs": [],
   "source": [
    "x_min_l2_apgd, x_list_l2_apgd = apgd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "monitor_apgd_l2 = monitor(apgd, pobj_l2, x_min_l2_apgd, args=(X, y, lbda, epsilon))\n",
    "monitor_apgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2, monitor_apgd_l2]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "lwvRwY5OEpbu",
    "outputId": "0bcf694d-898d-42a3-9ca8-1822667d98d4"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "x_min_l1_apgd, x_list_l1_apgd = apgd(x_init, grad_huber_loss, prox_l1, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "monitor_apgd_l1 = monitor(apgd, pobj_l1, x_min_l1_apgd, args=(X, y, lbda, epsilon))\n",
    "monitor_apgd_l1.run(x_init, grad_huber_loss, prox_l1, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l1, monitor_apgd_l1]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZOvEJISEpby"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 8:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the proximal coordinate descent (PCD) and add this solver to the monitoring plots for L1 and L2 regularized models.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Note:** You are welcome to try to use numba to get reasonable performance but don't spend too much time if you get weird numba errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uafUJ8va_TzN"
   },
   "outputs": [],
   "source": [
    "def grad_huber_loss_j(params, j, X=X, y=y, epsilon=epsilon):\n",
    "  #n = X.shape[0]\n",
    "  #w = params[1:]\n",
    "  #b = params[0]\n",
    "  \n",
    "  #grad = params.copy()\n",
    "  \n",
    "  #grad[1:] = - X[:, j].dot(grad_huber(y - X.dot(w) - b, epsilon))/n\n",
    "  #grad[0] = - np.sum(grad_huber(y - X.dot(w) - b, epsilon))/n\n",
    "\n",
    "  grad_b = - np.mean(grad_huber(y - np.dot(X, params[1:]) -  params[0]))\n",
    "  grad_j = - np.dot(X[:,j], grad_huber(y - np.dot(X, params[1:]) -  params[0])) / len(X)\n",
    "  \n",
    "  #return grad\n",
    "  return np.concatenate([grad_b,grad_j],axis=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4ZToTTLEpbz"
   },
   "outputs": [],
   "source": [
    "# #TODO\n",
    "# def pcd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "#         grad_args=(), prox_args=()):\n",
    "#     \"\"\"Proximal coordinate descent algorithm.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x_init : array, shape (n_parameters,)\n",
    "#         Parameters of the optimization problem.\n",
    "#     prox : callable\n",
    "#         The proximal operator of the regularization term.\n",
    "#     step : float\n",
    "#         The size of the gradient step done on the smooth term.\n",
    "#     n_iter : int\n",
    "#         The number of iterations.\n",
    "#     store_every : int\n",
    "#         At which frequency should the current iterated be remembered.\n",
    "#     grad_args : tuple\n",
    "#         Parameters to pass to grad.\n",
    "#     prox_args : tuple\n",
    "#         Parameters to pass to prox.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     params : array, shape (n_parameters,)\n",
    "#         The estimated parameters.\n",
    "#     x_list : list\n",
    "#         The list if x values along the iterations.\n",
    "#     \"\"\"\n",
    "\n",
    "    \n",
    "#     n_samples, n_features = X.shape\n",
    "    \n",
    "\n",
    "    \n",
    "#     L = (2/n_samples) * np.linalg.norm(X, axis=0)**2\n",
    "\n",
    "\n",
    "#     x = x_init.copy()\n",
    "#     x_list = []\n",
    "\n",
    "#     for t in range(n_iter):\n",
    "      \n",
    "      \n",
    "#         for j in range(n_features):\n",
    "          \n",
    "#             step = 1/L[j]          \n",
    "\n",
    "#             x[[0,j+1]] = prox(x[[0,j+1]] - step*grad(x, j, *grad_args), step, *prox_args)  \n",
    "           \n",
    "   \n",
    "#         if t % store_every == 0:\n",
    "#             x_list.append(x.copy())\n",
    "#     return x, x_list\n",
    "\n",
    "# # END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def pcd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Proximal coordinate descent algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_init : array, shape (n_parameters,)\n",
    "        Parameters of the optimization problem.\n",
    "    prox : callable\n",
    "        The proximal operator of the regularization term.\n",
    "    step : float\n",
    "        The size of the gradient step done on the smooth term.\n",
    "    n_iter : int\n",
    "        The number of iterations.\n",
    "    store_every : int\n",
    "        At which frequency should the current iterated be remembered.\n",
    "    grad_args : tuple\n",
    "        Parameters to pass to grad.\n",
    "    prox_args : tuple\n",
    "        Parameters to pass to prox.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    params : array, shape (n_parameters,)\n",
    "        The estimated parameters.\n",
    "    x_list : list\n",
    "        The list if x values along the iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    x = x_init.copy()\n",
    "    x_list = []\n",
    "\n",
    "    L = (2/n_samples) * np.linalg.norm(X, axis=0)**2\n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "          \n",
    "            step = 1/L[j]          \n",
    "      \n",
    "            x[[0,j+1]] = prox(x[[0,j+1]] - step * grad(x, *grad_args)[[0,j+1]], step, *prox_args)  \n",
    "           \n",
    "  # \n",
    "        if t % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFQiQPvf7vl9"
   },
   "outputs": [],
   "source": [
    "#x_min_l2_pcd, x_list_l2_pcd = pcd(x_init, grad_huber_loss_j, prox_l2, step, n_iter=10000, store_every=1,\n",
    "#                         grad_args = (X, y, b_hat, epsilon), prox_args = (lbda,))\n",
    "\n",
    "#x_min_l1_pcd, x_list_l1_pcd = pgd(x_init, grad_huber_loss_j, prox_l1, step, n_iter=10000, store_every=1,\n",
    "#                         grad_args = (X, y, b_hat, epsilon), prox_args = (lbda,))\n",
    "\n",
    "x_min_l2_pcd, x_list_l2_pcd = pcd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNjvKC5nGhrZ"
   },
   "outputs": [],
   "source": [
    "#plt.plot(x_min_l2_pcd, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJG5T507H0XP"
   },
   "outputs": [],
   "source": [
    "monitor_pcd_l2 = monitor(pcd, pobj_l2, x_min_l2_pcd, args=(X, y, lbda, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TES8WnTnH4e_"
   },
   "outputs": [],
   "source": [
    " monitor_pcd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                    grad_args=(X, y, epsilon), prox_args=(lbda,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "yCMvnFpEAqHX",
    "outputId": "d3b61058-5347-4b71-f4fd-5ab0c48b8009"
   },
   "outputs": [],
   "source": [
    "monitors = [monitor_pgd_l2, monitor_apgd_l2, monitor_pcd_l2]\n",
    "solvers = [\"PGD\", \"APGD\", \"PCD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iyyo5ZBAJXN"
   },
   "outputs": [],
   "source": [
    "x_min_l1_pcd, x_list_l1_pcd = pcd(x_init, grad_huber_loss, prox_l1, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_hCu2mSsYi4"
   },
   "outputs": [],
   "source": [
    "monitor_pcd_l1 = monitor(pcd, pobj_l1, x_min_l1_pcd, args=(X, y, lbda, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50v-CCettR2s"
   },
   "outputs": [],
   "source": [
    " monitor_pcd_l1.run(x_init, grad_huber_loss, prox_l1, step, n_iter,\n",
    "                    grad_args=(X, y, epsilon), prox_args=(lbda,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4mg7uLytbFt"
   },
   "outputs": [],
   "source": [
    "monitors = [monitor_pgd_l1, monitor_apgd_l1, monitor_pcd_l1]\n",
    "solvers = [\"PGD\", \"APGD\", \"PCD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULjGQKPbEpb3"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 9:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Compare the performance of the different solvers for different (simulated) problem sizes.\n",
    "    </li>\n",
    "    <li>\n",
    "        What solver would you recommend for what problem and using what regularization?\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyUxKitNz4ly"
   },
   "outputs": [],
   "source": [
    "#Small number of features \n",
    "X, y = make_regression(n_samples=500, n_features=10, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "# Set initial values of parameters to optimize\n",
    "x_init = np.zeros(n_features + 1)\n",
    "x_init[0] = np.mean(y)\n",
    "n_iter = 2000\n",
    "lbda = 1.\n",
    "epsilon = 1.\n",
    "\n",
    "x_min_l2, x_list_l2 = pgd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "\n",
    "# END TODO\n",
    "\n",
    "# Run PGD\n",
    "monitor_pgd_l2 = monitor(pgd, pobj_l2, x_min_l2, args=(X, y, lbda, epsilon))\n",
    "monitor_pgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "x_min_l2_apgd, x_list_l2_apgd = apgd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "monitor_apgd_l2 = monitor(apgd, pobj_l2, x_min_l2_apgd, args=(X, y, lbda, epsilon))\n",
    "monitor_apgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "x_min_l2_pcd, x_list_l2_pcd = pcd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "monitor_pcd_l2 = monitor(pcd, pobj_l2, x_min_l2_pcd, args=(X, y, lbda, epsilon))\n",
    "monitor_pcd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                    grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2, monitor_apgd_l2, monitor_pcd_l2]\n",
    "solvers = [\"PGD\", \"APGD\", \"PCD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKYHzUxXEpb5"
   },
   "source": [
    "## <font color=\"blue\"> Comments: </font>\n",
    "> We can see that when we reduce the number of features, PCD still converges more efficiently than PGD and APGD but the difference is less important.\n",
    "Indeed, higher is the dimension, more important is the gain brought by coordinate descent as it decomposes a large optimization problem into a sequence of one-dimensional problems (that we solve in one iteration of GD instead of solving it completely which reduces the cost of iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kz3HReQf8b6N"
   },
   "outputs": [],
   "source": [
    "#Small number of samples\n",
    "X, y = make_regression(n_samples=100, n_features=100, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "# Set initial values of parameters to optimize\n",
    "x_init = np.zeros(n_features + 1)\n",
    "x_init[0] = np.mean(y)\n",
    "n_iter = 2000\n",
    "lbda = 1.\n",
    "epsilon = 1.\n",
    "\n",
    "x_min_l2, x_list_l2 = pgd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "\n",
    "# END TODO\n",
    "\n",
    "# Run PGD\n",
    "monitor_pgd_l2 = monitor(pgd, pobj_l2, x_min_l2, args=(X, y, lbda, epsilon))\n",
    "monitor_pgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "x_min_l2_apgd, x_list_l2_apgd = apgd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "monitor_apgd_l2 = monitor(apgd, pobj_l2, x_min_l2_apgd, args=(X, y, lbda, epsilon))\n",
    "monitor_apgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "x_min_l2_pcd, x_list_l2_pcd = pcd(x_init, grad_huber_loss, prox_l2, step, n_iter=10000, store_every=1,\n",
    "                         grad_args = (X, y, epsilon), prox_args = (lbda,))\n",
    "\n",
    "monitor_pcd_l2 = monitor(pcd, pobj_l2, x_min_l2_pcd, args=(X, y, lbda, epsilon))\n",
    "monitor_pcd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                    grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2, monitor_apgd_l2, monitor_pcd_l2]\n",
    "solvers = [\"PGD\", \"APGD\",\"PCD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5KN18K5Ybcu"
   },
   "source": [
    "When we reduce the number of samples, PCD still converges faster but the convergence is less stable. For the three algorithms, we get closer to the optimum but the risk of overfitting is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDkX-QmzEpb7"
   },
   "source": [
    "# Part 3: Application\n",
    "\n",
    "You will now apply your solver to an environment dataset. Given 2 features:\n",
    "\n",
    " - LNOxEm log of hourly sum of NOx emission of cars on this motorway in arbitrary units.\n",
    " - sqrtWS Square root of wind speed [m/s].\n",
    "\n",
    "The objective is to predict:\n",
    "\n",
    " - log of hourly mean of NOx concentration in ambient air [ppb] next to a highly frequented motorway\n",
    "\n",
    "**Disclaimer:** This dataset is not huge and regularization makes little sense with so little features but it serves as a simple illustration. Also, don't be surprised if Huber loss offers little to no benefit. Again it's just an illustration.\n",
    "\n",
    "Let's first inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imhiGvjNEpb8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NOxEmissions.csv', index_col=0).drop(['julday'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZHmj4u3EpcB"
   },
   "source": [
    "Now let's extract `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPqjPSk4EpcE"
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, ['LNOxEm', 'sqrtWS']].values\n",
    "y = df['LNOx']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0t3WeCP8EpcI"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 1], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVyUMnioEpcM"
   },
   "source": [
    "In order to facilitate our experiment we're going to write a full scikit-learn estimator.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 9:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the `fit` method from the estimator in the next cell\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUOVZM9AEpcN"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class HuberRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"scikit-learn estimator for regression with a Huber loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : float\n",
    "        The regularization parameter\n",
    "    penalty : 'l1' | 'l2'\n",
    "        The type of regularization to use.\n",
    "    max_iter : int\n",
    "        The number of iterations / epochs to do on the data.\n",
    "    solver : 'pgd' | 'apgd' | 'pcd'\n",
    "        The type of algorithm to use.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : ndarray, (n_features,)\n",
    "        The weitghs w.\n",
    "    intercept_ : float\n",
    "        The intercept or bias term b.\n",
    "    \"\"\"\n",
    "    def __init__(self, lbda=1., penalty='l2', epsilon=1.,\n",
    "                 max_iter=2000, solver='pgd'):\n",
    "        self.lbda = lbda\n",
    "        self.penalty = penalty\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "        assert epsilon > 0.\n",
    "        assert self.penalty in ['l1', 'l2']\n",
    "        assert self.solver in ['pgd', 'apgd', 'pcd'] \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO\n",
    "    \n",
    "        x_init = np.zeros(n_features + 1)\n",
    "        x_init[0] = np.mean(y)\n",
    "        \n",
    "#         self.lbda = lbda\n",
    "#         self.max_iter = max_iter\n",
    "#         self.epsilon = epsilon\n",
    "        \n",
    "#         lbda = self.lbda\n",
    "#         epsilon = 10\n",
    "        \n",
    "        step = 1/smoothness_const_huber(X)\n",
    "            \n",
    "        \n",
    "        #if (self.solver==\"pgd\"):\n",
    "            \n",
    "        if (self.penalty==\"l1\"):\n",
    "            mtor = monitor(eval(self.solver), pobj_l1, x_min=None, args=(X, y, self.lbda, self.epsilon))\n",
    "            mtor.run(x_init, grad_huber_loss, prox_l1, step, self.max_iter,grad_args=(X, y, self.epsilon), prox_args=(self.lbda,))\n",
    "\n",
    "            x = mtor.x_list[-1]             \n",
    "\n",
    "        else:\n",
    "            mtor = monitor(eval(self.solver), pobj_l2, x_min=None, args=(X, y, self.lbda, self.epsilon))\n",
    "            mtor.run(x_init, grad_huber_loss, prox_l2, step, self.max_iter,grad_args=(X, y, self.epsilon), prox_args=(self.lbda,))\n",
    "\n",
    "            x = mtor.x_list[-1]                         \n",
    "\n",
    "        # END TODO\n",
    "        self.params_ = x\n",
    "        self.coef_ = x[1:]\n",
    "        self.intercept_ = x[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            The predicted target.\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.coef_) + self.intercept_\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Score using negative mean absolute error\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            The negative mean absolute error.\n",
    "            Negative to keep the semantic that higher is better.\n",
    "        \"\"\"\n",
    "        return -np.mean(np.abs(y - self.predict(X)))\n",
    "\n",
    "for solver in ['pgd', 'apgd','pcd']:\n",
    "    clf = HuberRegressor(lbda=1., penalty='l2', max_iter=1000, solver=solver)\n",
    "    clf.fit(X, y)\n",
    "    print('Solver with L2: %s   \\t-   MAE : %.5f' % (solver, -clf.score(X, y)))\n",
    "\n",
    "for solver in ['pgd', 'apgd','pcd']:\n",
    "    clf = HuberRegressor(lbda=1., penalty='l1', max_iter=1000, solver=solver)\n",
    "    clf.fit(X, y)\n",
    "    print('Solver with L1: %s   \\t-   MAE : %.5f' % (solver, -clf.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZ2TvNy1EpcS"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 10:</b>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Compare the cross-validation performance of your model (using `cross_val_score`) with a Ridge or Lasso regression models using as scoring metric the \"mean absolute error\" (MAE).\n",
    "        </li>\n",
    "        <li>\n",
    "            You will check that the Huber model matches Ridge when epsilon is large. Pay attention to how the loss is scaled in scikit-learn for Ridge (no normalization by 1/n_samples).\n",
    "        </li>\n",
    "        <li>\n",
    "            You will comment on the running time of your solver to reach their optimal prediction performance.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "To score your model with MAE using cross_val_score you need to pass as parameter `scoring='neg_mean_absolute_error'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgPeDAYhEpcW"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "kfold = 5\n",
    "lbda = 1.75\n",
    "\n",
    "scoring='neg_mean_absolute_error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nN2UmMjkF4ni"
   },
   "outputs": [],
   "source": [
    "# # Generate toy data\n",
    "# X, y = make_regression(n_samples=20, n_features=1, random_state=0,\n",
    "#                        noise=4.0, bias=10.0)\n",
    "\n",
    "# # Add an outlier\n",
    "# X[0, 0] = 2.\n",
    "# y[0] = 350\n",
    "# colors = ['r-', 'b-', 'y-', 'm-']\n",
    "# epsilon_values = [1.35, 1.5, 1.75, 1.9]\n",
    "# for k, epsilon in enumerate(epsilon_values):\n",
    "  \n",
    "#     params = huber_lbfgs_l2(X=X, y=y, lbda=0.0, epsilon=epsilon)\n",
    "\n",
    "#     w_hat = params[1:]\n",
    "#     b_hat = params[0]\n",
    "#     x = X[:, 0]\n",
    "#     y_pred = w_hat * x + b_hat\n",
    "    \n",
    "#     plt.plot(x, y_pred, colors[k], label=\"huber loss, %s\" % epsilon)\n",
    "\n",
    "\n",
    "# # Fit a ridge regressor to compare it to huber regressor.\n",
    "# ridge = Ridge(alpha=0.0, random_state=0, normalize=True)\n",
    "# ridge.fit(X, y)\n",
    "# coef_ridge = ridge.coef_\n",
    "# coef_ = ridge.coef_ * x + ridge.intercept_\n",
    "# plt.plot(x, coef_, 'g-', label=\"ridge regression\")\n",
    "\n",
    "# plt.title(\"Comparison of HuberRegressor vs Ridge\")\n",
    "# plt.xlabel(\"X\")\n",
    "# plt.ylabel(\"y\")\n",
    "# plt.legend(loc=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "u790-v-i-6Oh",
    "outputId": "1f21b4d1-6e0e-4a5f-b048-d1f5aa600f5d"
   },
   "outputs": [],
   "source": [
    "# huber = HuberRegressor(lbda=lbda, penalty='l2', max_iter=1000, epsilon=0.5)\n",
    "\n",
    "huber = HuberRegressor(lbda=lbda, penalty='l2', epsilon=1.,\n",
    "                 max_iter=2000, solver='pgd')\n",
    "\n",
    "huber_score_mean = -cross_val_score(huber,X,y,cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "print('Huber Regression:   MAE : %.5f  : ' % huber_score_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qcD80_IUQZ7E",
    "outputId": "3a944086-2361-4026-c6f5-800293bf327a"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=lbda, max_iter=1000, normalize=True)\n",
    "\n",
    "ridge_score_mean = -cross_val_score(ridge,X,y,cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "print('Ridge Regression:   MAE : %.5f  : ' % ridge_score_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96N2nTUazLe7"
   },
   "source": [
    "## <font color=\"blue\"> Epsilon large: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "a1aG9b1gQv5d",
    "outputId": "4a8e86b9-bd8f-4560-f068-fc6f04788157"
   },
   "outputs": [],
   "source": [
    "# epsilon=1.35 # there is something wrong, the score should decrease while increasing epsilon\n",
    "\n",
    "huber = HuberRegressor(lbda=lbda, max_iter=1000, epsilon=1.35)\n",
    "\n",
    "huber.fit(X, y)\n",
    "\n",
    "huber_score_mean = -cross_val_score(huber,X,y,cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "print('Huber Regression:   MAE : %.5f  : ' % huber_score_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7MrGux4PVZR"
   },
   "outputs": [],
   "source": [
    "# # Generate toy data.\n",
    "# rng = np.random.RandomState(0)\n",
    "# X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0,\n",
    "#                        bias=100.0)\n",
    "\n",
    "# # Add four strong outliers to the dataset.\n",
    "# X_outliers = rng.normal(0, 0.5, size=(4, 1))\n",
    "# y_outliers = rng.normal(0, 2.0, size=4)\n",
    "# X_outliers[:2, :] += X.max() + X.mean() / 4.\n",
    "# X_outliers[2:, :] += X.min() - X.mean() / 4.\n",
    "# y_outliers[:2] += y.min() - y.mean() / 4.\n",
    "# y_outliers[2:] += y.max() + y.mean() / 4.\n",
    "# X = np.vstack((X, X_outliers))\n",
    "# y = np.concatenate((y, y_outliers))\n",
    "# plt.plot(X, y, 'b.')\n",
    "\n",
    "# # Fit the huber regressor over a series of epsilon values.\n",
    "# colors = ['r-', 'b-', 'y-', 'm-']\n",
    "\n",
    "# x = np.linspace(X.min(), X.max(), 7)\n",
    "# epsilon_values = [1.35, 1.5, 1.75, 1.9]\n",
    "# for k, epsilon in enumerate(epsilon_values):\n",
    "#     huber = HuberRegressor(lbda=0.0, epsilon=epsilon)\n",
    "#     huber.fit(X, y)\n",
    "#     coef_ = huber.coef_ * x + huber.intercept_\n",
    "#     plt.plot(x, coef_, colors[k], label=\"huber loss, %s\" % epsilon)\n",
    "\n",
    "# # Fit a ridge regressor to compare it to huber regressor.\n",
    "# ridge = Ridge(alpha=0.0, random_state=0, normalize=True)\n",
    "# ridge.fit(X, y)\n",
    "# coef_ridge = ridge.coef_\n",
    "# coef_ = ridge.coef_ * x + ridge.intercept_\n",
    "# plt.plot(x, coef_, 'g-', label=\"ridge regression\")\n",
    "\n",
    "# plt.title(\"Comparison of HuberRegressor vs Ridge\")\n",
    "# plt.xlabel(\"X\")\n",
    "# plt.ylabel(\"y\")\n",
    "# plt.legend(loc=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0iY6xXhzQvV"
   },
   "source": [
    "#### <font color=\"green\"> Comments: </font>\n",
    "\n",
    "> The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust the huber regressor is to the outliers. So, when epsilon is large the Huber regressor is less robust to the outliers and will match Ridge regressor if we keep increasing the epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u-B4_B33ev3"
   },
   "source": [
    "## <font color=\"blue\"> Running time of the Solver: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GHKdcHOvzVCy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "project_bakary_sidibe_and_asadullah_adnan.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
